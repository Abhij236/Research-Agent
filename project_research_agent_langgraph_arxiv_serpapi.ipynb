{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8b3ac8",
   "metadata": {},
   "source": [
    "## Extracting Data from ArXiv into a Pandas DataFrame and Saving it as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adff5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Namespace for ArXiv's Atom-based XML format.\n",
    "ARXIV_NAMESPACE = '{http://www.w3.org/2005/Atom}'\n",
    "\n",
    "def extract_from_arxiv(search_query='cat:cs.AI', max_results=100, json_file_path='files/arxiv_dataset.json'):\n",
    "    \"\"\"\n",
    "    Fetches papers from the ArXiv API based on a search query, saves them as JSON, \n",
    "    and returns a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        search_query (str): The search query for ArXiv (default is 'cat:cs.AI').\n",
    "        max_results (int): The maximum number of results to retrieve (default is 100).\n",
    "        json_file_path (str): File path where JSON data will be saved.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the extracted paper information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the URL for the API request.\n",
    "    url = f'http://export.arxiv.org/api/query?search_query={search_query}&max_results={max_results}'\n",
    "    \n",
    "    # Send a GET request to the ArXiv API.\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the XML response.\n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    # Loop through each \"entry\" in the XML, representing a single paper.\n",
    "    for entry in root.findall(f'{ARXIV_NAMESPACE}entry'):\n",
    "        title = entry.find(f'{ARXIV_NAMESPACE}title').text.strip()\n",
    "        summary = entry.find(f'{ARXIV_NAMESPACE}summary').text.strip()\n",
    "\n",
    "        # Get the authors of the paper.\n",
    "        author_elements = entry.findall(f'{ARXIV_NAMESPACE}author')\n",
    "        authors = [author.find(f'{ARXIV_NAMESPACE}name').text for author in author_elements]\n",
    "\n",
    "        # Get the paper's URL.\n",
    "        paper_url = entry.find(f'{ARXIV_NAMESPACE}id').text\n",
    "        arxiv_id = paper_url.split('/')[-1]\n",
    "\n",
    "        # Check for the PDF link.\n",
    "        pdf_link = next((link.attrib['href'] for link in entry.findall(f'{ARXIV_NAMESPACE}link') \n",
    "                         if link.attrib.get('title') == 'pdf'), None)\n",
    "\n",
    "        papers.append({\n",
    "            'title': title,\n",
    "            'summary': summary,\n",
    "            'authors': authors,\n",
    "            'arxiv_id': arxiv_id,\n",
    "            'url': paper_url,\n",
    "            'pdf_link': pdf_link\n",
    "        })\n",
    "    \n",
    "    # Convert list into a pandas DataFrame.\n",
    "    df = pd.DataFrame(papers)\n",
    "    \n",
    "    # Save the DataFrame to a JSON file.\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(papers, f, ensure_ascii=False, indent=4)\n",
    "        print(f'Data saved to {json_file_path} ...')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_from_arxiv(max_results=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_name = 'files/arxiv_dataset.json'\n",
    "with  open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15dd25",
   "metadata": {},
   "source": [
    "## Downloading the Research Papers (PDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_pdfs(df, download_folder='files'):\n",
    "    \"\"\"\n",
    "    Downloads PDFs from URLs listed in the DataFrame and saves them to a specified folder. \n",
    "    The file names are stored in a new column 'pdf_file_name' in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a 'pdf_link' column with URLs to download.\n",
    "        download_folder (str): Path to the folder where PDFs will be saved (default is 'files').\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an additional 'pdf_file_name' column containing \n",
    "                      the paths of the downloaded PDF files or None if the download failed.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    \n",
    "    pdf_file_names = []\n",
    "    \n",
    "    # Loop through each row to download PDFs\n",
    "    for index, row in df.iterrows():\n",
    "        pdf_link = row['pdf_link']\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(pdf_link)\n",
    "            response.raise_for_status()\n",
    "    \n",
    "            file_name = os.path.join(download_folder, pdf_link.split('/')[-1]) + '.pdf'\n",
    "            pdf_file_names.append(file_name)\n",
    "    \n",
    "            # Save the downloaded PDF\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            print(f'PDF downloaded successfully and saved as {file_name}')\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Failed to download the PDF: {e}')\n",
    "            pdf_file_names.append(None)\n",
    "    \n",
    "    df['pdf_file_name'] = pdf_file_names\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ab7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = download_pdfs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c697e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1544de8",
   "metadata": {},
   "source": [
    "# Loading and Splitting PDF Files into Chunks, Expanding the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def load_and_chunk_pdf(pdf_file_name, chunk_size=512):\n",
    "    \"\"\"\n",
    "    Loads a PDF file and splits its content into chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        file (str): Path to the PDF file to be loaded.\n",
    "        chunk_size (int): The maximum size of each chunk in characters (default is 512).\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of document chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Loading and splitting into chunks: {pdf_file_name}')\n",
    "\n",
    "    # Load the content of the PDF\n",
    "    loader = PyPDFLoader(pdf_file_name)\n",
    "    data = loader.load()\n",
    "\n",
    "    # Split the content into chunks with slight overlap to preserve context\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=64)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_df(df):\n",
    "    \"\"\"\n",
    "    Expands each row in the DataFrame by splitting PDF documents into chunks.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'pdf_file_name', 'arxiv_id', 'title', 'summary', \n",
    "                           'authors', and 'url' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame where each row represents a chunk of the original document, \n",
    "                      with additional metadata such as chunk identifiers and relationships to \n",
    "                      adjacent chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    expanded_rows = []  # List to store expanded rows with chunk information\n",
    "\n",
    "    # Loop through each row in the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            chunks = load_and_chunk_pdf(row['pdf_file_name'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {row['pdf_file_name']}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Loop over the chunks and construct a new DataFrame row for each\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prechunk_id = i-1 if i > 0 else ''  # Preceding chunk ID\n",
    "            postchunk_id = i+1 if i < len(chunks) - 1 else ''  # Following chunk ID\n",
    "\n",
    "            expanded_rows.append({\n",
    "                'id': f\"{row['arxiv_id']}#{i}\",  # Unique chunk identifier\n",
    "                'title': row['title'],\n",
    "                'summary': row['summary'],\n",
    "                'authors': row['authors'],\n",
    "                'arxiv_id': row['arxiv_id'],\n",
    "                'url': row['url'],\n",
    "                'chunk': chunk.page_content,  # Text content of the chunk\n",
    "                'prechunk_id': '' if i == 0 else f\"{row['arxiv_id']}#{prechunk_id}\",  # Previous chunk ID\n",
    "                'postchunk_id': '' if i == len(chunks) - 1 else f\"{row['arxiv_id']}#{postchunk_id}\"  # Next chunk ID\n",
    "            })\n",
    "\n",
    "    # Return a new expanded DataFrame\n",
    "    return pd.DataFrame(expanded_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bff91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = expand_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9431bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b889128",
   "metadata": {},
   "source": [
    "## Building a Knowledge Base for the RAG System Using Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load the API keys from .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6926c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "\n",
    "# Check if 'OPENAI_API_KEY' is set; prompt if not\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY') or getpass('OpenAI API key: ')\n",
    "\n",
    "# Initialize the OpenAIEncoder with a specific model\n",
    "encoder = OpenAIEncoder(name='text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbcd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder('hello hallo hola salut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = len(encoder('hello hallo hola salut')[0])\n",
    "dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397509ba",
   "metadata": {},
   "source": [
    "## Creating a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Check if 'PINECONE_API_KEY' is set; prompt if not\n",
    "api_key = os.getenv('PINECONE_API_KEY') or getpass('Pinecone API key: ')\n",
    "\n",
    "# Initialize the Pinecone client\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# Define the serverless specification for Pinecone (AWS region 'us-east-1')\n",
    "spec = ServerlessSpec(\n",
    "    cloud='aws', \n",
    "    region='us-east-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define the name of the index\n",
    "index_name = 'langgraph-research-agent'\n",
    "\n",
    "# Check if the index exists; create it if it doesn't\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=dims,  # Embedding dimension (1536)\n",
    "        metric='cosine',\n",
    "        spec=spec  # Cloud provider and region specification\n",
    "    )\n",
    "\n",
    "    # Wait until the index is fully initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Add a short delay before checking the stats\n",
    "time.sleep(1)\n",
    "\n",
    "# View the index statistics\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a7e20f",
   "metadata": {},
   "source": [
    "## Populating the Knowledge Base and Uploading it to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd869de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "data = expanded_df\n",
    "batch_size = 64  # Set batch size\n",
    "\n",
    "# Loop through the data in batches, using tqdm for a progress bar\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i + batch_size)  # Define batch endpoint\n",
    "    batch = data[i:i_end].to_dict(orient='records')  # Slice data into a batch\n",
    "\n",
    "    # Extract metadata for each chunk in the batch\n",
    "    metadata = [{\n",
    "        'arxiv_id': r['arxiv_id'],\n",
    "        'title': r['title'],\n",
    "        'chunk': r['chunk'],\n",
    "    } for r in batch]\n",
    "    \n",
    "    # Generate unique IDs for each chunk\n",
    "    ids = [r['id'] for r in batch]\n",
    "    \n",
    "    # Extract the chunk content\n",
    "    chunks = [r['chunk'] for r in batch]\n",
    "    \n",
    "    # Convert chunks into embeddings\n",
    "    embeds = encoder(chunks)\n",
    "    \n",
    "    # Upload embeddings, IDs, and metadata to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the index statistics.\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f9b22",
   "metadata": {},
   "source": [
    "## Implementing the ArXiv fetch langchain tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758dda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Compile a regular expression pattern to find the abstract in the HTML response\n",
    "abstract_pattern = re.compile(\n",
    "    r'<blockquote class=\"abstract mathjax\">\\s*<span class=\"descriptor\">Abstract:</span>\\s*(.*?)\\s*</blockquote>',\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "@tool('fetch_arxiv')\n",
    "def fetch_arxiv(arxiv_id: str) -> str:\n",
    "    '''Fetches the abstract from an ArXiv paper given its ArXiv ID.\n",
    "\n",
    "    Args:\n",
    "        arxiv_id (str): The ArXiv paper ID.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted abstract text from the ArXiv paper.\n",
    "    '''\n",
    "\n",
    "    res = requests.get(f'https://arxiv.org/abs/{arxiv_id}')\n",
    "    \n",
    "    re_match = abstract_pattern.search(res.text)\n",
    "\n",
    "    return re_match.group(1) if re_match else 'Abstract not found.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa510b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the ArXiv paper ID and invoking the tool with that ID.\n",
    "arxiv_id = '1706.03762'\n",
    "output = fetch_arxiv.invoke(input={'arxiv_id': arxiv_id})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3197e56",
   "metadata": {},
   "source": [
    "## Implementing the Web Search Tools with Google SerpAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load the API keys from .env\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set up the SerpAPI request parameters, including the API key.\n",
    "serpapi_params = {\n",
    "    'engine': 'google',  \n",
    "    'api_key': os.getenv('SERPAPI_KEY') or getpass('SerpAPI key: ')  # Get the API key securely.\n",
    "}\n",
    "\n",
    "# Perform a Google search for the keyword \"water\" and limit the results to 5.\n",
    "search = GoogleSearch({\n",
    "    **serpapi_params,\n",
    "    'q': 'water',\n",
    "    'num': 5\n",
    "})\n",
    "\n",
    "\n",
    "# Extract the main search results from the API response.\n",
    "results = search.get_dict().get('organic_results', [])\n",
    "\n",
    "# Format the search results for readability.\n",
    "formatted_results = '\\n---\\n'.join(\n",
    "    ['\\n'.join([x['title'], x['snippet'], x['link']]) for x in results]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fe399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from serpapi import GoogleSearch\n",
    "\n",
    "# Define the 'web_search' tool using the '@tool' decorator.\n",
    "@tool('web_search')\n",
    "def web_search(query: str) -> str:\n",
    "    '''Finds general knowledge information using a Google search.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string of the top search results, including title, snippet, and link.\n",
    "    '''\n",
    "    \n",
    "    search = GoogleSearch({\n",
    "        **serpapi_params,  \n",
    "        'q': query,        \n",
    "        'num': 5         \n",
    "    })\n",
    "   \n",
    "    results = search.get_dict().get('organic_results', [])\n",
    "    formatted_results = '\\n---\\n'.join(\n",
    "        ['\\n'.join([x['title'], x['snippet'], x['link']]) for x in results]\n",
    "    )\n",
    "    \n",
    "    # Return the formatted results or a 'No results found.' message if no results exist.\n",
    "    return formatted_results if results else 'No results found.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3665ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the 'web_search' tool with the query 'water on mars'\n",
    "output = web_search.invoke(input={'query': 'water on mars'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c43b6e9",
   "metadata": {},
   "source": [
    "## Creating RAG Tools for Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rag_contexts(matches: list) -> str:\n",
    "    '''Formats the retrieved context matches into a readable string format.\n",
    "\n",
    "    Args:\n",
    "        matches (list): A list of matched documents with metadata.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string of document titles, chunks, and ArXiv IDs.\n",
    "    '''\n",
    "    formatted_results = []\n",
    "    \n",
    "    # Loop through each match and extract its metadata.\n",
    "    for x in matches:\n",
    "        text = (\n",
    "            f\"Title: {x['metadata']['title']}\\n\"\n",
    "            f\"Chunk: {x['metadata']['chunk']}\\n\"\n",
    "            f\"ArXiv ID: {x['metadata']['arxiv_id']}\\n\"\n",
    "        )\n",
    "        # Append each formatted string to the results list.\n",
    "        formatted_results.append(text)\n",
    "    \n",
    "    # Join all the individual formatted strings into one large string.\n",
    "    return '\\n---\\n'.join(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def rag_search_filter(query: str, arxiv_id: str) -> str:\n",
    "    '''Finds information from the ArXiv database using a natural language query and a specific ArXiv ID.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query in natural language.\n",
    "        arxiv_id (str): The ArXiv ID of the specific paper to filter by.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string of relevant document contexts.\n",
    "    '''\n",
    "    \n",
    "    # Encode the query into a vector representation.\n",
    "    xq = encoder([query])\n",
    "    \n",
    "    # Perform a search on the Pinecone index, filtering by ArXiv ID.\n",
    "    xc = index.query(vector=xq, top_k=6, include_metadata=True, filter={'arxiv_id': arxiv_id})\n",
    "    \n",
    "    # Format and return the search results.\n",
    "    return format_rag_contexts(xc['matches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3322290",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool('rag_search')\n",
    "def rag_search(query: str) -> str:\n",
    "    '''Finds specialist information on AI using a natural language query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query in natural language.\n",
    "    \n",
    "    Returns:\n",
    "        str: A formatted string of relevant document contexts.\n",
    "    '''\n",
    "    \n",
    "    # Encode the query into a vector representation.\n",
    "    xq = encoder([query])\n",
    "    \n",
    "    # Perform a broader search without filtering by ArXiv ID.\n",
    "    xc = index.query(vector=xq, top_k=5, include_metadata=True)\n",
    "    \n",
    "    # Format and return the search results.\n",
    "    return format_rag_contexts(xc['matches'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
