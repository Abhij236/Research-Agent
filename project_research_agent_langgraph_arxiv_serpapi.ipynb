{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8b3ac8",
   "metadata": {},
   "source": [
    "## Extracting Data from ArXiv into a Pandas DataFrame and Saving it as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3adff5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Namespace for ArXiv's Atom-based XML format.\n",
    "ARXIV_NAMESPACE = '{http://www.w3.org/2005/Atom}'\n",
    "\n",
    "def extract_from_arxiv(search_query='cat:cs.AI', max_results=100, json_file_path='files/arxiv_dataset.json'):\n",
    "    \"\"\"\n",
    "    Fetches papers from the ArXiv API based on a search query, saves them as JSON, \n",
    "    and returns a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        search_query (str): The search query for ArXiv (default is 'cat:cs.AI').\n",
    "        max_results (int): The maximum number of results to retrieve (default is 100).\n",
    "        json_file_path (str): File path where JSON data will be saved.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the extracted paper information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the URL for the API request.\n",
    "    url = f'http://export.arxiv.org/api/query?search_query={search_query}&max_results={max_results}'\n",
    "    \n",
    "    # Send a GET request to the ArXiv API.\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the XML response.\n",
    "    root = ET.fromstring(response.content)\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    # Loop through each \"entry\" in the XML, representing a single paper.\n",
    "    for entry in root.findall(f'{ARXIV_NAMESPACE}entry'):\n",
    "        title = entry.find(f'{ARXIV_NAMESPACE}title').text.strip()\n",
    "        summary = entry.find(f'{ARXIV_NAMESPACE}summary').text.strip()\n",
    "\n",
    "        # Get the authors of the paper.\n",
    "        author_elements = entry.findall(f'{ARXIV_NAMESPACE}author')\n",
    "        authors = [author.find(f'{ARXIV_NAMESPACE}name').text for author in author_elements]\n",
    "\n",
    "        # Get the paper's URL.\n",
    "        paper_url = entry.find(f'{ARXIV_NAMESPACE}id').text\n",
    "        arxiv_id = paper_url.split('/')[-1]\n",
    "\n",
    "        # Check for the PDF link.\n",
    "        pdf_link = next((link.attrib['href'] for link in entry.findall(f'{ARXIV_NAMESPACE}link') \n",
    "                         if link.attrib.get('title') == 'pdf'), None)\n",
    "\n",
    "        papers.append({\n",
    "            'title': title,\n",
    "            'summary': summary,\n",
    "            'authors': authors,\n",
    "            'arxiv_id': arxiv_id,\n",
    "            'url': paper_url,\n",
    "            'pdf_link': pdf_link\n",
    "        })\n",
    "    \n",
    "    # Convert list into a pandas DataFrame.\n",
    "    df = pd.DataFrame(papers)\n",
    "    \n",
    "    # Save the DataFrame to a JSON file.\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(papers, f, ensure_ascii=False, indent=4)\n",
    "        print(f'Data saved to {json_file_path} ...')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e59c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_from_arxiv(max_results=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_name = 'files/arxiv_dataset.json'\n",
    "with  open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df.sample(n=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
